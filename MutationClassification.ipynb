{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the place from where the network can be run and tested. Everything relating to the setup can be done here, while \"specialised\" code should be delegated to its own python file. Ideally the process that is run through here will then later be adapted to a 'main' execution file in Python that can be run from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the (probably) necessary imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import os\n",
    "\n",
    "# Probable project code structure\n",
    "from project_code.utils import preprocessing\n",
    "from project_code.data.zebrafish_data_module import *\n",
    "from project_code.networks.rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard for easy debugging.\n",
    "import tensorboard\n",
    "\n",
    "# This might be a little different for Pytorch lightning.\n",
    "# For one, the logs are stored in lightning_logs.\n",
    "# For two, I don't know if we should still remove them in between.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs\n",
    "\n",
    "# If you run this notebook locally, you can also access Tensorbaord at 127.0.0.1:6006 now.\n",
    "\n",
    "# Clean up old logs.\n",
    "if os.path.isdir('./lightning_logs/'):\n",
    "  import shutil\n",
    "  shutil.rmtree('lightning_logs/')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default 'log_dir' is \"lightning_logs\"\n",
    "writer = SummaryWriter('lightning_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to define a simple DataModule and two Datasets\n",
    "# the Datasets will contain generated sequences of white\n",
    "# noise with different means. The idea is that the model\n",
    "# should be able to differentiate between them and update\n",
    "# their belief system. If we can see this, then we know\n",
    "# that the model works.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 mean1=1,\n",
    "                 mean2=5,\n",
    "                 num_samples=1000,\n",
    "                 sequence_length=1000):\n",
    "        self.mean1 = mean1\n",
    "        self.mean2 = mean2\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Generate the sequences and put them in an array.\n",
    "        self.sequences = []\n",
    "        self.sequences_target = []\n",
    "        output_shape = (self.sequence_length, 1)\n",
    "        for i in range(self.num_samples):\n",
    "            # Make sure that half is of one mean and half of the other.\n",
    "            if i % 2 == 0:\n",
    "                sequence = np.random.normal(mean1, 1, (self.sequence_length, 1))\n",
    "                target = np.ones(output_shape)\n",
    "            else:\n",
    "                sequence = np.random.normal(mean2, 1, (self.sequence_length, 1))\n",
    "                target = np.zeros(output_shape)\n",
    "            \n",
    "            # Use a low-pass filter on the noise such that\n",
    "            # the confidence values also don't change as\n",
    "            # quickly (and we can hopefully see better\n",
    "            # integration over time).\n",
    "            sequence = np.convolve(np.squeeze(sequence),\n",
    "                                   np.ones(10).T/10,\n",
    "                                   mode='same')\n",
    "            sequence = np.expand_dims(sequence, axis=1)\n",
    "\n",
    "            self.sequences.append(sequence)\n",
    "            self.sequences_target.append(target)\n",
    "\n",
    "            if i < 2:\n",
    "                plt.figure()\n",
    "                plt.plot(sequence)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequences[idx], self.sequences_target[idx])\n",
    "\n",
    "class TestDataModule(pl.LightningDataModule):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 mean1=1,\n",
    "                 mean2=5, \n",
    "                 batch_size=32, \n",
    "                 num_samples=1000, \n",
    "                 sequence_length=1000):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.mean1 = mean1\n",
    "        self.mean2 = mean2\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        # Make assignments here (val/train/test split).\n",
    "        # Called on every process in Distributed Data Processing.\n",
    "        \n",
    "        dataset = TestDataset(self.mean1, \n",
    "                              self.mean2, \n",
    "                              self.num_samples, \n",
    "                              self.sequence_length)\n",
    "        \n",
    "        # Like in the RestingDataset we go for a 80, 10, 10 split.\n",
    "        num_train = round(self.num_samples * 0.8)\n",
    "        num_val = round(self.num_samples * 0.1)\n",
    "        num_test = round(self.num_samples * 0.1)\n",
    "        \n",
    "        # We could be missing samples due to rounding.\n",
    "        # In that case we add it to the test set.\n",
    "        num_test = num_test + self.num_samples \\\n",
    "                   - (num_train + num_val + num_test)\n",
    "        \n",
    "        self.train, self.val, self.test = \\\n",
    "                torch.utils.data.random_split(dataset, \\\n",
    "                [num_train, num_val, num_test])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run through the whole process using Pytorch Lightning.\n",
    "\n",
    "# Initialise the model\n",
    "model = MutationNet()\n",
    "\n",
    "# The model needs to use double (instead of float)\n",
    "model = model.double()\n",
    "\n",
    "# Initialise the data.\n",
    "data_module = ZebrafishDataModule(batch_size=1, sampling_frequency=100)\n",
    "#data_module = TestDataModule(mean1=1, mean2=1.50, num_samples=10000, sequence_length=100)\n",
    "\n",
    "# Train the model.\n",
    "trainer = pl.Trainer(max_epochs=25)\n",
    "#trainer = pl.Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model\n",
    "#trainer.test(datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintialise trainer such that the logs will be stored in a new version.\n",
    "trainer = pl.Trainer()\n",
    "trainer.test(model, datamodule=TestDataModule(mean1=1, mean2=1.35))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
