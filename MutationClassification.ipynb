{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the place from where the network can be run and tested. Everything relating to the setup can be done here, while \"specialised\" code should be delegated to its own python file. Ideally the process that is run through here will then later be adapted to a 'main' execution file in Python that can be run from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the (probably) necessary imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import os\n",
    "\n",
    "# For Early Stopping Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Ray tune for tuning the hyperparameters\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "        TuneReportCheckpointCallback\n",
    "\n",
    "# Probable project code structure\n",
    "from project_code.utils import preprocessing\n",
    "from project_code.data.zebrafish_data_module import *\n",
    "from project_code.networks.rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard for easy debugging.\n",
    "import tensorboard\n",
    "\n",
    "# This might be a little different for Pytorch lightning.\n",
    "# For one, the logs are stored in lightning_logs.\n",
    "# For two, I don't know if we should still remove them in between.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs\n",
    "\n",
    "# If you run this notebook locally, you can also access Tensorbaord at 127.0.0.1:6006 now.\n",
    "\n",
    "# Clean up old logs.\n",
    "if os.path.isdir('./lightning_logs/'):\n",
    "  import shutil\n",
    "  shutil.rmtree('lightning_logs/')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default 'log_dir' is \"lightning_logs\"\n",
    "writer = SummaryWriter('lightning_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to define a simple DataModule and two Datasets\n",
    "# the Datasets will contain generated sequences of white\n",
    "# noise with different means. The idea is that the model\n",
    "# should be able to differentiate between them and update\n",
    "# their belief system. If we can see this, then we know\n",
    "# that the model works.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 mean1=1,\n",
    "                 mean2=5,\n",
    "                 num_samples=1000,\n",
    "                 sequence_length=1000):\n",
    "        self.mean1 = mean1\n",
    "        self.mean2 = mean2\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Generate the sequences and put them in an array.\n",
    "        self.sequences = []\n",
    "        self.sequences_target = []\n",
    "        output_shape = (self.sequence_length, 1)\n",
    "        for i in range(self.num_samples):\n",
    "            # Make sure that half is of one mean and half of the other.\n",
    "            if i % 2 == 0:\n",
    "                sequence = np.random.normal(mean1, 1, (self.sequence_length, 1))\n",
    "                target = np.ones(output_shape)\n",
    "            else:\n",
    "                sequence = np.random.normal(mean2, 1, (self.sequence_length, 1))\n",
    "                target = np.zeros(output_shape)\n",
    "            \n",
    "            # Use a low-pass filter on the noise such that\n",
    "            # the confidence values also don't change as\n",
    "            # quickly (and we can hopefully see better\n",
    "            # integration over time).\n",
    "            sequence = np.convolve(np.squeeze(sequence),\n",
    "                                   np.ones(10).T/10,\n",
    "                                   mode='same')\n",
    "            sequence = np.expand_dims(sequence, axis=1)\n",
    "\n",
    "            self.sequences.append(sequence)\n",
    "            self.sequences_target.append(target)\n",
    "\n",
    "            if i < 2:\n",
    "                plt.figure()\n",
    "                plt.plot(sequence)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequences[idx], self.sequences_target[idx])\n",
    "\n",
    "class TestDataModule(pl.LightningDataModule):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 mean1=1,\n",
    "                 mean2=5, \n",
    "                 batch_size=32, \n",
    "                 num_samples=1000, \n",
    "                 sequence_length=1000):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.mean1 = mean1\n",
    "        self.mean2 = mean2\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        # Make assignments here (val/train/test split).\n",
    "        # Called on every process in Distributed Data Processing.\n",
    "        \n",
    "        dataset = TestDataset(self.mean1, \n",
    "                              self.mean2, \n",
    "                              self.num_samples, \n",
    "                              self.sequence_length)\n",
    "        \n",
    "        # Like in the RestingDataset we go for a 80, 10, 10 split.\n",
    "        num_train = round(self.num_samples * 0.8)\n",
    "        num_val = round(self.num_samples * 0.1)\n",
    "        num_test = round(self.num_samples * 0.1)\n",
    "        \n",
    "        # We could be missing samples due to rounding.\n",
    "        # In that case we add it to the test set.\n",
    "        num_test = num_test + self.num_samples \\\n",
    "                   - (num_train + num_val + num_test)\n",
    "        \n",
    "        self.train, self.val, self.test = \\\n",
    "                torch.utils.data.random_split(dataset, \\\n",
    "                [num_train, num_val, num_test])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do hyperparameter tuning with Ray Tune.\n",
    "\n",
    "# Define logging callback.\n",
    "tune_report_callback = TuneReportCallback({\n",
    "    'loss': 'val_loss'\n",
    "}, on='validation_end')\n",
    "\n",
    "def train_with_tune(config, num_epochs=40, num_gpus=0):\n",
    "    # Initialise the model\n",
    "    model = MutationNet(config)\n",
    "\n",
    "    # The model needs to use double (instead of float)\n",
    "    model = model.double()\n",
    "\n",
    "    # Initialise the data.\n",
    "    #data_module = ZebrafishDataModule(batch_size=1, sampling_frequency=100)\n",
    "    data_module = TestDataModule(mean1=1, mean2=1.5, num_samples=1000, sequence_length=1000)\n",
    "\n",
    "    # Train the model.\n",
    "    trainer = pl.Trainer(callbacks=[tune_report_callback],\n",
    "                         progress_bar_refresh_rate = 0,\n",
    "                        )\n",
    "    #trainer = pl.Trainer(fast_dev_run=True)\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "def tune_model_asha(num_samples=10, num_epochs=10, gpus_per_trial=0):\n",
    "    config = {\n",
    "        'input_size': 1,\n",
    "        'hidden_size': tune.lograndint(1, 100),\n",
    "        'num_layers': tune.lograndint(1, 10),\n",
    "        'learning_rate': tune.loguniform(1e-4, 1e-1)\n",
    "    }\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t = num_epochs,\n",
    "        grace_period = 1,\n",
    "        reduction_factor = 2)\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=['hidden_size', 'num_layers', 'learning_rate'],\n",
    "        metric_columns=['loss', 'training_iteration'])\n",
    "    \n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_with_tune,\n",
    "            num_epochs = num_epochs,\n",
    "            num_gpus = gpus_per_trial),\n",
    "        resources_per_trial = {\n",
    "            'cpu': 1,\n",
    "            'gpu': gpus_per_trial\n",
    "        },\n",
    "        metric = 'loss',\n",
    "        mode = 'min',\n",
    "        config = config,\n",
    "        num_samples = num_samples,\n",
    "        checkpoint_at_end = True,\n",
    "        scheduler = scheduler,\n",
    "        progress_reporter = reporter,\n",
    "        name = 'tune_model_asha')\n",
    "    \n",
    "    print('Best hyperparameters found were: ', analysis.best_config)\n",
    "    \n",
    "    # Return the best config.\n",
    "    return analysis.best_config\n",
    "\n",
    "config = tune_model_asha(num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For individual training\n",
    "\n",
    "# Specify the parameters for the model.\n",
    "#config = {\n",
    "#    'input_size': 1,\n",
    "#    'hidden_size': 3,\n",
    "#    'num_layers': 1,\n",
    "#    'learning_rate': 1e-3\n",
    "#}\n",
    "\n",
    "# Initialise the model with the previously found best config.\n",
    "model = MutationNet(config)\n",
    "\n",
    "# The model needs to use double (instead of float)\n",
    "model = model.double()\n",
    "\n",
    "# Initialise the data.\n",
    "data_module = ZebrafishDataModule(batch_size=1, sampling_frequency=100)\n",
    "#data_module = TestDataModule(mean1=1, mean2=1.50, num_samples=10000, sequence_length=100)\n",
    "\n",
    "# Train the model.\n",
    "trainer = pl.Trainer(max_epochs=40, callbacks=[EarlyStopping(monitor=\"val_loss\")])\n",
    "#trainer = pl.Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model\n",
    "#trainer.test(datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintialise trainer such that the logs will be stored in a new version.\n",
    "trainer = pl.Trainer()\n",
    "trainer.test(model, datamodule=TestDataModule(mean1=1, mean2=1.35))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
