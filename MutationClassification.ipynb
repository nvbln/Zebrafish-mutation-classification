{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the place from where the network can be run and tested. Everything relating to the setup can be done here, while \"specialised\" code should be delegated to its own python file. Ideally the process that is run through here will then later be adapted to a 'main' execution file in Python that can be run from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the (probably) necessary imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import os\n",
    "\n",
    "# For Early Stopping Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Ray tune for tuning the hyperparameters\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "        TuneReportCheckpointCallback\n",
    "\n",
    "# Probable project code structure\n",
    "from project_code.utils import preprocessing\n",
    "from project_code.data.zebrafish_data_module import *\n",
    "from project_code.networks.rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard for easy debugging.\n",
    "import tensorboard\n",
    "\n",
    "# This might be a little different for Pytorch lightning.\n",
    "# For one, the logs are stored in lightning_logs.\n",
    "# For two, I don't know if we should still remove them in between.\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs\n",
    "\n",
    "# If you run this notebook locally, you can also access Tensorbaord at 127.0.0.1:6006 now.\n",
    "\n",
    "# Clean up old logs.\n",
    "if os.path.isdir('./lightning_logs/'):\n",
    "  import shutil\n",
    "  shutil.rmtree('lightning_logs/')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default 'log_dir' is \"lightning_logs\"\n",
    "writer = SummaryWriter('lightning_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to define a simple DataModule and two Datasets\n",
    "# the Datasets will contain generated sequences of white\n",
    "# noise with different means. The idea is that the model\n",
    "# should be able to differentiate between them and update\n",
    "# their belief system. If we can see this, then we know\n",
    "# that the model works.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 mean1=1,\n",
    "                 mean2=5,\n",
    "                 num_samples=1000,\n",
    "                 sequence_length=1000):\n",
    "        self.mean1 = mean1\n",
    "        self.mean2 = mean2\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Generate the sequences and put them in an array.\n",
    "        self.sequences = []\n",
    "        self.sequences_target = []\n",
    "        #output_shape = (self.sequence_length, 1)\n",
    "        for i in range(self.num_samples):\n",
    "            # Make sure that half is of one mean and half of the other.\n",
    "            if i % 2 == 0:\n",
    "                sequence = np.random.normal(mean1, 0.25, (self.sequence_length, 1))\n",
    "                #target = np.ones(output_shape)\n",
    "                target = np.ones((1,))\n",
    "            else:\n",
    "                sequence = np.random.normal(mean2, 0.25, (self.sequence_length, 1))\n",
    "                #target = np.zeros(output_shape)\n",
    "                target = np.zeros((1,))\n",
    "                \n",
    "            # Normalise the trace.\n",
    "            #norm = np.linalg.norm(sequence)\n",
    "            #sequence = sequence / norm\n",
    "            \n",
    "            # Use a low-pass filter on the noise such that\n",
    "            # the confidence values also don't change as\n",
    "            # quickly (and we can hopefully see better\n",
    "            # integration over time).\n",
    "            sequence = np.convolve(np.squeeze(sequence),\n",
    "                                   np.ones(10).T/10,\n",
    "                                   mode='same')\n",
    "            sequence = np.expand_dims(sequence, axis=1)\n",
    "\n",
    "            self.sequences.append(sequence)\n",
    "            self.sequences_target.append(target)\n",
    "\n",
    "            if i < 2:\n",
    "                plt.figure()\n",
    "                plt.plot(sequence)\n",
    "                plt.show()\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequences[idx], self.sequences_target[idx])\n",
    "\n",
    "class TestDataModule(pl.LightningDataModule):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 mean1=1,\n",
    "                 mean2=5, \n",
    "                 batch_size=32, \n",
    "                 num_samples=1000, \n",
    "                 sequence_length=1000):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.mean1 = mean1\n",
    "        self.mean2 = mean2\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        # Make assignments here (val/train/test split).\n",
    "        # Called on every process in Distributed Data Processing.\n",
    "        \n",
    "        dataset = TestDataset(self.mean1, \n",
    "                              self.mean2, \n",
    "                              self.num_samples, \n",
    "                              self.sequence_length)\n",
    "        \n",
    "        # Like in the RestingDataset we go for a 80, 10, 10 split.\n",
    "        num_train = round(self.num_samples * 0.8)\n",
    "        num_val = round(self.num_samples * 0.1)\n",
    "        num_test = round(self.num_samples * 0.1)\n",
    "        \n",
    "        # We could be missing samples due to rounding.\n",
    "        # In that case we add it to the test set.\n",
    "        num_test = num_test + self.num_samples \\\n",
    "                   - (num_train + num_val + num_test)\n",
    "        \n",
    "        self.train, self.val, self.test = \\\n",
    "                torch.utils.data.random_split(dataset, \\\n",
    "                [num_train, num_val, num_test])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, num_workers=4, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, num_workers=4, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, num_workers=4, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "# Utility function\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return(rho, phi)\n",
    "\n",
    "# 0 degrees alines with the x axis, increasing degrees\n",
    "# means going rightward.\n",
    "def pol2cart(rho, phi):\n",
    "    # Convert degrees to radians.\n",
    "    phi = np.deg2rad(phi)\n",
    "    \n",
    "    # Convert polar coordinates to cartesian coordinates.\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    \n",
    "    return(x, y)\n",
    "\n",
    "# Update the orientation while staying between 0 and 360.\n",
    "def update_orientation(orientation, angle):\n",
    "    #print('Angle: {}'.format(angle))\n",
    "    while angle >= 360 or angle < 0:\n",
    "        if angle >= 360:\n",
    "            angle = angle - 360\n",
    "        else:\n",
    "            angle = angle + 360\n",
    "    #print('New orientation: {}'.format((orientation + angle) % 360))\n",
    "    return (orientation + angle) % 360\n",
    "\n",
    "# Modelling function\n",
    "def generate_fish_trace(beta_parameters, exp_length, frequency):\n",
    "    a, b = beta_parameters\n",
    "    \n",
    "    # Initialise an empty trace.\n",
    "    simulated_trace = np.empty((exp_length * frequency, 2))\n",
    "    angle = np.empty((exp_length * frequency, 1))\n",
    "    x = simulated_trace[:, 0]\n",
    "    y = simulated_trace[:, 1]\n",
    "    \n",
    "    # Set model parameters\n",
    "    bout_freq = 1 # 1 bout per second.\n",
    "    avg_bout_vel = 0.5 # 0.5mm per second.\n",
    "    vel_duration = 1 # the fish will keep a velocity > 0 for 1 second after the bout.\n",
    "    avg_peak_velocity = 0.5 # at the start of the bout, the average velocity is 0.5mm.\n",
    "    \n",
    "    # Initialise the fish at a random location.\n",
    "    # Random location is a Gaussian around 0.\n",
    "    x[0], y[0] = np.random.normal(0, 3), np.random.normal(0, 3)\n",
    "    \n",
    "    # Initialise the first bout.\n",
    "    bout_countdown = int(np.random.normal(\n",
    "                        loc=bout_freq * frequency,\n",
    "                        scale=(frequency / 2) / bout_freq))\n",
    "    if bout_countdown < 0:\n",
    "        bout_countdown = 0\n",
    "        \n",
    "    # Initialise current velocity\n",
    "    current_velocity = 0\n",
    "    \n",
    "    # Initialise current fish orientation (in degrees).\n",
    "    # Bout angles are relative, so we need to keep\n",
    "    # track of the orientation.\n",
    "    current_orientation = np.random.random_sample() * 360\n",
    "    #current_orientation = 0\n",
    "    angle[0] = current_orientation\n",
    "    \n",
    "    # Simulate every measurement.\n",
    "    for i in range(1, exp_length * frequency):\n",
    "        # If the bout_countdown is 0, then we bout.\n",
    "        if bout_countdown == 0:\n",
    "            current_velocity = np.random.normal(\n",
    "                                loc=avg_peak_velocity, \n",
    "                                scale=avg_peak_velocity*0.3)\n",
    "            current_orientation = update_orientation(\n",
    "                                    current_orientation,\n",
    "                                    (np.random.beta(a, b) * 360) - 180)\n",
    "            #current_velocity = np.random.beta(a, b)\n",
    "            #current_orientation = update_orientation(\n",
    "            #                        current_orientation,\n",
    "            #                        np.random.normal(\n",
    "            #                            scale=90)\n",
    "            #                      )\n",
    "            #if b == 20:\n",
    "            #    current_orientation = update_orientation(current_orientation, 90)\n",
    "            #    current_orientation = current_orientation + 90\n",
    "            #else:\n",
    "            #    current_orientation = update_orientation(current_orientation, -90)\n",
    "            #    current_orientation = current_orientation - 90\n",
    "            \n",
    "            # We have to set a new timer.\n",
    "            bout_countdown = int(np.random.normal(\n",
    "                                loc=bout_freq * frequency,\n",
    "                                scale=(frequency / 2) / bout_freq))\n",
    "            if bout_countdown < 0:\n",
    "                bout_countdown = 0\n",
    "        else:\n",
    "            bout_countdown = bout_countdown - 1\n",
    "            \n",
    "            # Update velocity, since it goes down over time.\n",
    "            # We want to go back to 0 in 1 second on average.\n",
    "            # We'll keep it simple and do it linearly.\n",
    "            current_velocity = current_velocity \\\n",
    "                    - (avg_peak_velocity / frequency)\n",
    "            if current_velocity < 0:\n",
    "                current_velocity = 0\n",
    "        \n",
    "        # Update position.\n",
    "        moved_x, moved_y = pol2cart(current_velocity, \n",
    "                                    current_orientation)\n",
    "        \n",
    "        x[i] = x[i-1] + moved_x\n",
    "        y[i] = y[i-1] + moved_y\n",
    "        \n",
    "        angle[i] = current_orientation\n",
    "        \n",
    "        # Add white noise to the measurement.\n",
    "        #x[i] = x[i] + np.random.normal(0, 0.1)\n",
    "        #y[i] = y[i] + np.random.normal(0, 0.1)\n",
    "        \n",
    "    #plt.figure()\n",
    "    #plt.plot(angle)\n",
    "    #plt.show()\n",
    "        \n",
    "    return simulated_trace\n",
    "        \n",
    "# Run the simulation and plot the trace.\n",
    "simulated_trace = generate_fish_trace((10, 10), 60, 100)\n",
    "\n",
    "segments = np.zeros((simulated_trace.shape[0] - 1, 2, 2))\n",
    "segments[:, 0, 0] = simulated_trace[:-1, 0]\n",
    "segments[:, 0, 1] = simulated_trace[:-1, 1]\n",
    "segments[:, 1, 0] = simulated_trace[1:, 0]\n",
    "segments[:, 1, 1] = simulated_trace[1:, 1]\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "lc = LineCollection(segments, cmap='viridis')\n",
    "#norm = plt.Normalize(exp.behavior_log.t.min(), exp.behavior_log.t.max())\n",
    "#lc.set_array(norm(exp.behavior_log.t.tolist()))\n",
    "lc.set_array(np.linspace(0, 60, len(simulated_trace)-1))\n",
    "line = axs.add_collection(lc)\n",
    "axs.set_xlim(simulated_trace[:, 0].min(), simulated_trace[:, 0].max())\n",
    "axs.set_ylim(simulated_trace[:, 1].min(), simulated_trace[:, 1].max())\n",
    "plt.colorbar(line)\n",
    "plt.title('Simulated fish 1')\n",
    "plt.show()\n",
    "\n",
    "# Run the simulation and plot the trace.\n",
    "simulated_trace = generate_fish_trace((30, 15), 60, 100)\n",
    "\n",
    "segments = np.zeros((simulated_trace.shape[0] - 1, 2, 2))\n",
    "segments[:, 0, 0] = simulated_trace[:-1, 0]\n",
    "segments[:, 0, 1] = simulated_trace[:-1, 1]\n",
    "segments[:, 1, 0] = simulated_trace[1:, 0]\n",
    "segments[:, 1, 1] = simulated_trace[1:, 1]\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "lc = LineCollection(segments, cmap='viridis')\n",
    "#norm = plt.Normalize(exp.behavior_log.t.min(), exp.behavior_log.t.max())\n",
    "#lc.set_array(norm(exp.behavior_log.t.tolist()))\n",
    "lc.set_array(np.linspace(0, 60, len(simulated_trace)-1))\n",
    "line = axs.add_collection(lc)\n",
    "axs.set_xlim(simulated_trace[:, 0].min(), simulated_trace[:, 0].max())\n",
    "axs.set_ylim(simulated_trace[:, 1].min(), simulated_trace[:, 1].max())\n",
    "plt.colorbar(line)\n",
    "plt.title('Simulated fish 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to define a simple DataModule and two Datasets\n",
    "# the Datasets will contain generated traces of zebrafish\n",
    "# with different behaviour. The idea is that the model\n",
    "# should be able to differentiate between them and update\n",
    "# their belief system. If we can see this, then we know\n",
    "# that the model works.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ModelledZebrafishDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_samples=100,\n",
    "                 exp_length=60):\n",
    "        \n",
    "        # Number of fish.\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Length of the experiment in seconds.\n",
    "        self.exp_length = exp_length\n",
    "        \n",
    "        # We \"measure\" at a frequency of 100Hz.\n",
    "        frequency = 25\n",
    "        \n",
    "        # Generate the traces and put them in an array.\n",
    "        self.traces = []\n",
    "        self.traces_target = []\n",
    "        for i in range(self.num_samples):\n",
    "            # Make sure that half is of one type of behaviour and half of the other.\n",
    "            if i % 2 == 0:\n",
    "                trace = generate_fish_trace((10, 10), self.exp_length, frequency)\n",
    "                #trace = np.ones((self.exp_length * frequency, 1))\n",
    "                target = np.ones((1,))\n",
    "            else:\n",
    "                trace = generate_fish_trace((30, 15), self.exp_length, frequency)\n",
    "                #trace = np.zeros((self.exp_length * frequency, 1)) + 0.01\n",
    "                target = np.zeros((1,))\n",
    "                \n",
    "            # Normalise the trace.\n",
    "            #norm = np.linalg.norm(trace)\n",
    "            #trace = trace / norm\n",
    "            \n",
    "            extracted_angles = np.empty((len(trace), 1))\n",
    "            \n",
    "            last_angle = 0\n",
    "            relative_angle = 0\n",
    "            \n",
    "            simulated_trace = trace\n",
    "\n",
    "            for i in range(1, len(simulated_trace)):\n",
    "                current_x = simulated_trace[i, 0]\n",
    "                current_y = simulated_trace[i, 1]\n",
    "\n",
    "                previous_x = simulated_trace[i-1, 0]\n",
    "                previous_y = simulated_trace[i-1, 1]\n",
    "\n",
    "                # Calculate the relative change in position.\n",
    "                rel_x = current_x - previous_x\n",
    "                rel_y = current_y - previous_y\n",
    "\n",
    "                # Calculate the polar coordinates of this.\n",
    "                rho, phi = cart2pol(rel_x, rel_y)\n",
    "                \n",
    "                # Convert the angle to degrees for readability.\n",
    "                phi = np.rad2deg(phi)\n",
    "\n",
    "                # Round Phi, since we might have a bit of noise.\n",
    "                phi = round(phi)\n",
    "                # The angle calculation contains some noise, so it can occur\n",
    "                # that the angle goes from -180 to 180 or the other way around.\n",
    "                # In order not to extract weird angles, check for this.\n",
    "                if phi != last_angle and not ((phi == 180 and last_angle == -180) or phi == -180 and last_angle == -180) and rho > 0:\n",
    "                    # Take the (180, -180) range into account.\n",
    "                    if phi - last_angle > 180:\n",
    "                        relative_angle = -1 * ((phi - last_angle) % 180)\n",
    "                    elif phi - last_angle < -180:\n",
    "                        relative_angle = -1 * ((phi - last_angle) % -180)\n",
    "                    else:\n",
    "                        relative_angle = phi - last_angle\n",
    "                    #print('Last angle: {}'.format(last_angle))\n",
    "                    last_angle = phi\n",
    "                    #print('Phi: {}'.format(phi))\n",
    "                    #print('Last angle: {}'.format(np.rad2deg(last_angle)))\n",
    "                    #print('Relative angle: {}'.format(relative_angle))\n",
    "        \n",
    "                extracted_angles[i] = relative_angle\n",
    "\n",
    "            self.traces.append(simulated_trace)\n",
    "            #self.traces.append(extracted_angles)\n",
    "            self.traces_target.append(target)\n",
    "            \n",
    "            #plt.figure()\n",
    "            #plt.plot(extracted_angles)\n",
    "            #plt.show()\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.traces[idx], self.traces_target[idx])\n",
    "\n",
    "class ModelledZebrafishDataModule(pl.LightningDataModule):\n",
    "   \n",
    "    def __init__(self,\n",
    "                 batch_size=32, \n",
    "                 num_samples=100, \n",
    "                 exp_length=60):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.exp_length = exp_length\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        # Make assignments here (val/train/test split).\n",
    "        # Called on every process in Distributed Data Processing.\n",
    "        \n",
    "        dataset = ModelledZebrafishDataset(self.num_samples, \n",
    "                                           self.exp_length)\n",
    "        \n",
    "        # Like in the RestingDataset we go for a 80, 10, 10 split.\n",
    "        num_train = round(self.num_samples * 0.8)\n",
    "        num_val = round(self.num_samples * 0.1)\n",
    "        num_test = round(self.num_samples * 0.1)\n",
    "        \n",
    "        # We could be missing samples due to rounding.\n",
    "        # In that case we add it to the test set.\n",
    "        num_test = num_test + self.num_samples \\\n",
    "                   - (num_train + num_val + num_test)\n",
    "        \n",
    "        self.train, self.val, self.test = \\\n",
    "                torch.utils.data.random_split(dataset, \\\n",
    "                [num_train, num_val, num_test])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, num_workers=4, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, num_workers=4, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, num_workers=4, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do hyperparameter tuning with Ray Tune.\n",
    "\n",
    "# Define logging callback.\n",
    "tune_report_callback = TuneReportCallback({\n",
    "    'loss': 'val_loss'\n",
    "}, on='validation_end')\n",
    "\n",
    "def train_with_tune(config, num_epochs=40, num_gpus=0):\n",
    "    # Initialise the model\n",
    "    model = MutationNet(config)\n",
    "\n",
    "    # The model needs to use double (instead of float)\n",
    "    model = model.double()\n",
    "\n",
    "    # Initialise the data.\n",
    "    #data_module = ZebrafishDataModule(batch_size=1, sampling_frequency=100)\n",
    "    data_module = TestDataModule(mean1=1, mean2=1.5, num_samples=1000, sequence_length=1000)\n",
    "\n",
    "    # Train the model.\n",
    "    trainer = pl.Trainer(callbacks=[tune_report_callback],\n",
    "                         progress_bar_refresh_rate = 0,\n",
    "                        )\n",
    "    #trainer = pl.Trainer(fast_dev_run=True)\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "def tune_model_asha(num_samples=10, num_epochs=10, gpus_per_trial=0):\n",
    "    config = {\n",
    "        'model_type': tune.choice(['LSTM', 'GRU']),\n",
    "        'input_size': 1,\n",
    "        'hidden_size': tune.lograndint(1, 100),\n",
    "        'num_layers': tune.lograndint(1, 10),\n",
    "        'learning_rate': tune.loguniform(1e-4, 1e-1)\n",
    "    }\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t = num_epochs,\n",
    "        grace_period = 1,\n",
    "        reduction_factor = 2)\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=['model_type', 'hidden_size', 'num_layers', 'learning_rate'],\n",
    "        metric_columns=['loss', 'training_iteration'])\n",
    "    \n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_with_tune,\n",
    "            num_epochs = num_epochs,\n",
    "            num_gpus = gpus_per_trial),\n",
    "        resources_per_trial = {\n",
    "            'cpu': 1,\n",
    "            'gpu': gpus_per_trial\n",
    "        },\n",
    "        metric = 'loss',\n",
    "        mode = 'min',\n",
    "        config = config,\n",
    "        num_samples = num_samples,\n",
    "        checkpoint_at_end = True,\n",
    "        scheduler = scheduler,\n",
    "        progress_reporter = reporter,\n",
    "        name = 'tune_model_asha')\n",
    "    \n",
    "    print('Best hyperparameters found were: ', analysis.best_config)\n",
    "    \n",
    "    # Return the best config.\n",
    "    return analysis.best_config\n",
    "\n",
    "config = tune_model_asha(num_samples=3, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For individual training\n",
    "\n",
    "# Specify the parameters for the model.\n",
    "config = {\n",
    "    'model_type': 'LSTM',\n",
    "    'input_size': 1,\n",
    "    'hidden_size': 10,\n",
    "    'num_layers': 1,\n",
    "    'learning_rate': 1e-1\n",
    "}\n",
    "\n",
    "# Initialise the model with the previously found best config.\n",
    "model = MutationNet(config)\n",
    "\n",
    "# The model needs to use double (instead of float)\n",
    "model = model.double()\n",
    "\n",
    "# Initialise the data.\n",
    "#data_module = ZebrafishDataModule(batch_size=1)\n",
    "data_module = TestDataModule(mean1=1, mean2=5, num_samples=100, sequence_length=1000)\n",
    "#data_module = ModelledZebrafishDataModule(batch_size=32, num_samples=1000, exp_length=60)\n",
    "\n",
    "# Train the model.\n",
    "trainer = pl.Trainer(max_epochs=15, callbacks=[EarlyStopping(monitor=\"val_loss\", patience=30)], accelerator=\"gpu\", devices=1)\n",
    "#trainer = pl.Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Test the model\n",
    "#trainer.test(datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.random.normal(5, 0.25, (1000, 2))\n",
    "\n",
    "# Run the simulation and plot the trace.\n",
    "#simulated_trace, angles = generate_fish_trace1((3.2, 20), 60, 100)\n",
    "\n",
    "x = np.expand_dims(sequence, axis=0)\n",
    "print(x.shape)\n",
    "x = x.astype(np.float32)\n",
    "model.float()\n",
    "a, b = model(torch.Tensor(x).float())\n",
    "\n",
    "print(a)\n",
    "\n",
    "a = a.detach().numpy()\n",
    "a = np.squeeze(a)\n",
    "\n",
    "#prediction = prediction.detach().numpy()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(a, color='red')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.plot(sequence)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "print(sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintialise trainer such that the logs will be stored in a new version.\n",
    "#trainer = pl.Trainer()\n",
    "#trainer.test(model, datamodule=TestDataModule(mean1=1, mean2=1.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Beta distributions\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Plot beta distribution for the first fish.\n",
    "a, b = 10, 10\n",
    "mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "ax.plot((x * 360) - 180, beta.pdf(x, a, b),\n",
    "        'r-', lw=5, alpha=0.6, label='beta(10, 10)')\n",
    "\n",
    "# Plot beta distribution for the second fish.\n",
    "a, b = 30, 15\n",
    "mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n",
    "\n",
    "ax.plot((x * 360) - 180, beta.pdf(x, a, b),\n",
    "        'b-', lw=5, alpha=0.6, label='beta(30, 15)')\n",
    "\n",
    "plt.xlabel('Angle in degrees')\n",
    "plt.ylabel('Sample weight')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "plt.title('Bout angle beta distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Modelling function\n",
    "def generate_fish_trace1(beta_parameters, exp_length, frequency):\n",
    "    a, b = beta_parameters\n",
    "    \n",
    "    # Initialise an empty trace.\n",
    "    simulated_trace = np.empty((exp_length * frequency, 2))\n",
    "    angle = np.empty((exp_length * frequency, 1))\n",
    "    x = simulated_trace[:, 0]\n",
    "    y = simulated_trace[:, 1]\n",
    "    \n",
    "    # Set model parameters\n",
    "    bout_freq = 1 # 1 bout per second.\n",
    "    avg_bout_vel = 0.5 # 0.5mm per second.\n",
    "    vel_duration = 1 # the fish will keep a velocity > 0 for 1 second after the bout.\n",
    "    avg_peak_velocity = 0.5 # at the start of the bout, the average velocity is 0.5mm.\n",
    "    \n",
    "    # Initialise the fish at a random location.\n",
    "    # Random location is a Gaussian around 0.\n",
    "    x[0], y[0] = np.random.normal(0, 3), np.random.normal(0, 3)\n",
    "    \n",
    "    # Initialise the first bout.\n",
    "    bout_countdown = int(np.random.normal(\n",
    "                        loc=bout_freq * frequency,\n",
    "                        scale=(frequency / 2) / bout_freq))\n",
    "    if bout_countdown < 0:\n",
    "        bout_countdown = 0\n",
    "        \n",
    "    # Initialise current velocity\n",
    "    current_velocity = 0\n",
    "    \n",
    "    # Initialise current fish orientation (in degrees).\n",
    "    # Bout angles are relative, so we need to keep\n",
    "    # track of the orientation.\n",
    "    current_orientation = np.random.random_sample() * 360\n",
    "    #current_orientation = 0\n",
    "    angle[0] = current_velocity\n",
    "    \n",
    "    # Simulate every measurement.\n",
    "    for i in range(1, exp_length * frequency):\n",
    "        # If the bout_countdown is 0, then we bout.\n",
    "        if bout_countdown == 0:\n",
    "            current_velocity = np.random.normal(\n",
    "                                loc=avg_peak_velocity, \n",
    "                                scale=avg_peak_velocity*0.3)\n",
    "            current_orientation = update_orientation(\n",
    "                                    current_orientation,\n",
    "                                    (np.random.beta(a, b) * 360) - 180)\n",
    "            #current_velocity = np.random.beta(a, b)\n",
    "            #current_orientation = update_orientation(\n",
    "            #                        current_orientation,\n",
    "            #                        np.random.normal(\n",
    "            #                            scale=90)\n",
    "            #)\n",
    "             \n",
    "            \n",
    "            #if b == 20:\n",
    "            #    current_orientation = update_orientation(current_orientation, 90)\n",
    "            #    current_orientation = current_orientation + 90\n",
    "            #else:\n",
    "            #    current_orientation = update_orientation(current_orientation, -90)\n",
    "            #    current_orientation = current_orientation - 90\n",
    "            \n",
    "            # We have to set a new timer.\n",
    "            bout_countdown = int(np.random.normal(\n",
    "                                loc=bout_freq * frequency,\n",
    "                                scale=(frequency / 2) / bout_freq))\n",
    "            if bout_countdown < 0:\n",
    "                bout_countdown = 0\n",
    "        else:\n",
    "            bout_countdown = bout_countdown - 1\n",
    "            \n",
    "            # Update velocity, since it goes down over time.\n",
    "            # We want to go back to 0 in 1 second on average.\n",
    "            # We'll keep it simple and do it linearly.\n",
    "            current_velocity = current_velocity \\\n",
    "                    - (avg_peak_velocity / frequency)\n",
    "            if current_velocity < 0:\n",
    "                current_velocity = 0\n",
    "        \n",
    "        # Update position.\n",
    "        moved_x, moved_y = pol2cart(current_velocity, \n",
    "                                    current_orientation)\n",
    "        \n",
    "        x[i] = x[i-1] + moved_x\n",
    "        y[i] = y[i-1] + moved_y\n",
    "        \n",
    "        angle[i] = current_orientation\n",
    "        \n",
    "        # Add white noise to the measurement.\n",
    "        #x[i] = x[i] + np.random.normal(0, 0.1)\n",
    "        #y[i] = y[i] + np.random.normal(0, 0.1)\n",
    "        \n",
    "    #plt.figure()\n",
    "    #plt.plot(angle)\n",
    "    #plt.show()\n",
    "        \n",
    "    return simulated_trace, angle\n",
    "        \n",
    "# Run the simulation and plot the trace.\n",
    "simulated_trace, angles = generate_fish_trace1((10, 10), 60, 100)\n",
    "\n",
    "#simulated_trace, angles = generate_fish_trace1((20, 5.2), 60, 100)\n",
    "extracted_angles = []\n",
    "extracted_velocities = []\n",
    "\n",
    "last_angle = 0\n",
    "relative_angle = 0\n",
    "\n",
    "extracted_angles = np.empty((len(simulated_trace), 1))\n",
    "            \n",
    "last_angle = 0\n",
    "relative_angle = 0\n",
    "\n",
    "for i in range(1, len(simulated_trace)):\n",
    "    current_x = simulated_trace[i, 0]\n",
    "    current_y = simulated_trace[i, 1]\n",
    "\n",
    "    previous_x = simulated_trace[i-1, 0]\n",
    "    previous_y = simulated_trace[i-1, 1]\n",
    "\n",
    "    # Calculate the relative change in position.\n",
    "    rel_x = current_x - previous_x\n",
    "    rel_y = current_y - previous_y\n",
    "\n",
    "    # Calculate the polar coordinates of this.\n",
    "    rho, phi = cart2pol(rel_x, rel_y)\n",
    "    \n",
    "    # Convert the angle to degrees for readability.\n",
    "    phi = np.rad2deg(phi)\n",
    "    \n",
    "    # Round Phi, since we might have a bit of noise.\n",
    "    phi = round(phi)\n",
    "    # The angle calculation contains some noise, so it can occur\n",
    "    # that the angle goes from -180 to 180 or the other way around.\n",
    "    # In order not to extract weird angles, check for this.\n",
    "    if phi != last_angle and not ((phi == 180 and last_angle == -180) or phi == -180 and last_angle == -180) and rho > 0:\n",
    "        # Take the (180, -180) range into account.\n",
    "        if phi - last_angle > 180:\n",
    "            relative_angle = -1 * ((phi - last_angle) % 180)\n",
    "        elif phi - last_angle < -180:\n",
    "            relative_angle = -1 * ((phi - last_angle) % -180)\n",
    "        else:\n",
    "            relative_angle = phi - last_angle\n",
    "        #print('Last angle: {}'.format(last_angle))\n",
    "        last_angle = phi\n",
    "        #print('Phi: {}'.format(phi))\n",
    "        #print('Last angle: {}'.format(np.rad2deg(last_angle)))\n",
    "        #print('Relative angle: {}'.format(relative_angle))\n",
    "        \n",
    "        \n",
    "    extracted_angles[i] = relative_angle\n",
    "\n",
    "#norm = np.linalg.norm(simulated_trace)\n",
    "#simulated_trace = simulated_trace / norm\n",
    "\n",
    "x = np.expand_dims(simulated_trace, axis=0)\n",
    "print(f'Shape of x: {x.shape}')\n",
    "x = x.astype(np.float32)\n",
    "model.float()\n",
    "a, b = model(torch.Tensor(x).float())\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "a = b.detach().numpy()\n",
    "a = np.squeeze(a)\n",
    "\n",
    "#prediction = prediction.detach().numpy()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Confidence value', color=color)\n",
    "ax1.plot(np.linspace(0, 60, 6000), a, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Angle in degrees', color=color)\n",
    "ax2.set_ylim([-180, 180])\n",
    "ax2.plot(np.linspace(0, 60, 6000), extracted_angles, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Confidence values for extracted bout angles')\n",
    "\n",
    "# Make the plots readable on the presentation.\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "if True:\n",
    "    segments = np.zeros((simulated_trace.shape[0] - 1, 2, 2))\n",
    "    segments[:, 0, 0] = simulated_trace[:-1, 0]\n",
    "    segments[:, 0, 1] = simulated_trace[:-1, 1]\n",
    "    segments[:, 1, 0] = simulated_trace[1:, 0]\n",
    "    segments[:, 1, 1] = simulated_trace[1:, 1]\n",
    "\n",
    "    fig, axs = plt.subplots()\n",
    "    lc = LineCollection(segments, cmap='viridis')\n",
    "    #norm = plt.Normalize(exp.behavior_log.t.min(), exp.behavior_log.t.max())\n",
    "    #lc.set_array(norm(exp.behavior_log.t.tolist()))\n",
    "    lc.set_array(np.linspace(0, 60, len(simulated_trace)-1))\n",
    "    line = axs.add_collection(lc)\n",
    "    axs.set_xlim(simulated_trace[:, 0].min(), simulated_trace[:, 0].max())\n",
    "    axs.set_ylim(simulated_trace[:, 1].min(), simulated_trace[:, 1].max())\n",
    "    plt.colorbar(line)\n",
    "    plt.title('Simulated fish 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm = model.lstm\n",
    "\n",
    "for i in lstm.named_parameters():\n",
    "    print(i)\n",
    "    \n",
    "print('break')\n",
    "    \n",
    "for i in model.linear.named_parameters():\n",
    "    print(i)\n",
    "    \n",
    "print(model.linear.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to extract the angles from the trace.\n",
    "simulated_trace, angles = generate_fish_trace1((20, 5.2), 60, 100)\n",
    "extracted_angles = []\n",
    "extracted_velocities = []\n",
    "\n",
    "last_angle = 0\n",
    "relative_angle = 0\n",
    "\n",
    "for i in range(1, len(simulated_trace)):\n",
    "    current_x = simulated_trace[i, 0]\n",
    "    current_y = simulated_trace[i, 1]\n",
    "    \n",
    "    previous_x = simulated_trace[i-1, 0]\n",
    "    previous_y = simulated_trace[i-1, 1]\n",
    "    \n",
    "    # Calculate the relative change in position.\n",
    "    rel_x = current_x - previous_x\n",
    "    rel_y = current_y - previous_y\n",
    "    \n",
    "    # Calculate the polar coordinates of this.\n",
    "    rho, phi = cart2pol(rel_x, rel_y)\n",
    "    if phi != last_angle:\n",
    "        relative_angle = phi - last_angle\n",
    "        last_angle = phi\n",
    "    extracted_angles.append(relative_angle)\n",
    "    #extracted_angles.append(phi-extracted_angles[-1])\n",
    "    extracted_velocities.append(rho)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.rad2deg(extracted_angles))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(extracted_velocities)\n",
    "plt.show()\n",
    "\n",
    "extracted_angles2 = extracted_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trace1 = simulated_trace\n",
    "#extracted_angles2 = extracted_angles\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(extracted_angles1)\n",
    "plt.plot(extracted_angles2, color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
